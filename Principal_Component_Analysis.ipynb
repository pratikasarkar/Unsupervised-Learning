{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Principal Component Analysis",
      "provenance": [],
      "authorship_tag": "ABX9TyOBx9E3ny1Zq1aHEtWrqfL5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikasarkar/Unsupervised-Learning/blob/master/Principal_Component_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q96dqUZYGDdO",
        "colab_type": "text"
      },
      "source": [
        "# **Principal Component Analysis (PCA)**\n",
        "### It is a data transformation technique. The final objective of PCA is DIMENSIONALITY REDUCTION.\n",
        "The general myth, that the idea of dimensionality reduction means that PCA will drop some of the weak features, is WRONG.\n",
        "\n",
        "Consider we have some features which are highly significant in our data, out of these some are highly correlated to out target variable and some a weakly correlated. There is a chance that those weakly correlated features have high correlation among themselves, eventually adding redundancy and multicolliearity to our models. This multicolliearity effect is the NOISE in PCA. So the intention of PCA is to reduce that noise.\n",
        "\n",
        "***Lets have a look at it into with an example more intuitively.***\n",
        "\n",
        "Consider we have 15 features in a dataset. Out these there are 2 features which are highly correlated to each other.\n",
        "\n",
        "So if we plot a histogram of these 2 features, a majority of the part will be overlapping as they are highly correlated. You can see the plot below :\n",
        "\n",
        "**FIGURE 1 :**   \n",
        "![alt text](https://i.ibb.co/k1TRbNv/Q9GDn.png)\n",
        "\n",
        "The important information which these 2 features will provide us will be those data points which are not redundant/non overlapping and lie at the extreme ends of the histograms as shown in the plot below:\n",
        "\n",
        "**FIGURE 2 :**\n",
        "\n",
        "![alt text](https://i.ibb.co/7VR6ZNy/Q9GDn1.png)\n",
        "\n",
        "This is the information which we cannot afford to lose because the information in these records are responsible for significantly differentiating the 2 features from each other.\n",
        "\n",
        "On the other hand, there is a lot of redundant/overlapping data due to high correlation in these 2 features which gives rise to multicollinearity. The following plot represents this redundant data : \n",
        "\n",
        "**FIGURE 3 :**\n",
        "\n",
        "![alt text](https://i.ibb.co/nnPmXYr/Q9GDn2.png)\n",
        "\n",
        "This redundant data causes multicolliearity. So even if we lose some data from this part, there won't be much information loss.\n",
        "\n",
        "So we can say that we are not sacrificing the feature but we are sacrificing the information content at the low frequency/low variance zone.\n",
        "\n",
        "*Consider an example of predicting a cancerous patient. We can afford to lose some information from a healthy patient(eg. normal bp range/normal rbc-wbc level) as it will not affect our prediction result to an extreme level. But on the other hand we can't affort to lose the information from a cancerous patient(eg. abnormal bp range/abnormal rbc-wbc level), bcoz it could lead to a loss of extensive information and our model could behave unexpectedly.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT0RKRaV7fN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
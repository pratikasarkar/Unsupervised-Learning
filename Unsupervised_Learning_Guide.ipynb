{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised Learning Guide",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGsACwYhoarvS4BWHB6w9q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikasarkar/Unsupervised-Learning/blob/master/Unsupervised_Learning_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9LwLtsigGOE",
        "colab_type": "text"
      },
      "source": [
        "***Please UpVote if you like the work!!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s2VDw-3Jszl",
        "colab_type": "text"
      },
      "source": [
        "# **Distance for continous values :** \n",
        "\n",
        "1. Euclidean Distance = np.sqrt( (a1-a2)^2 + (b1-b2)^2 )\n",
        "2. Manhattan distance = np.abs(a1-a2) + np.abs(b1-b2)\n",
        "3. Cosine Distance = 1 - [ ( (a1xa2)+(b1xb2) ) / ( np.sqrt(a1^2 + b1^2) x np.sqrt(a2^2 + b2^2) ) ]\n",
        "#### ***Cosine distance = 1 - Cosine_Similarity***\n",
        "![alt text](https://neo4j.com/docs/graph-algorithms/current/images/cosine-similarity.png)\n",
        "\n",
        "*Distance based models should involve mandatory scaling of data.*\n",
        "\n",
        "A general thumbrule for scaling - \n",
        "\n",
        "1. Use \"StandardScalar/zscore\" when majority of our data is continous.\n",
        "  *  Range of \"StandardScalar/zscore\" is between **-3 to +3**.\n",
        "  *  StandardScalar/zscore = (Value - mean_data)/std_data\n",
        "2. Use \"MinMaxScalar\" when majority of our data is categorical or there are a 50% continous and 50% categorical variables.\n",
        "  *  Range of \"MinMaxScalar\" is between **0 to +1**.\n",
        "  *  MinMaxScalar = (Value - min_value)/(max_val - min_val)\n",
        "3. \"MaxNormalization\" is generally used to scale pixel of image data (Convert from 0-255 grayscale to 0-1 scale)\n",
        "  *  Range of \"MaxNormalization\" is between **minimum_value to +1**.\n",
        "  *  StandardScalar/zscore = Value/max_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9Yym41RBtz1",
        "colab_type": "text"
      },
      "source": [
        "# **Distance feature for Categorical values :**\n",
        "\n",
        "1. ***Jaccard distance = 1 - Jaccard_Similarity***\n",
        "\n",
        "Assume we have two vectors A and B. To calculate the Jaccard similarity we use the following formula:\n",
        "\n",
        "Jaccard similarity = M11 / ( M01 + M10 + M11 )\n",
        "\n",
        "In other words, the Jaccard similarity coefficient measures the number of attributes where A and B are both 1, divided by the number of attributes where A and B are dissimilar, plus the number of attributes where they are both 1.\n",
        "\n",
        "**Jaccard distance = 1 - M11 / ( M01 + M10 + M11 )\n",
        " = (M01 + M10) / ( M01 + M10 + M11 )**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xlw5s2JS8Oy",
        "colab_type": "text"
      },
      "source": [
        "# **KMeans Clustering**\n",
        "![alt text](https://jhui.github.io/assets/ml/kmeans.png)\n",
        "\n",
        "#### Visualization link : https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n",
        "\n",
        "K-Means starts by randomly defining k centroids. From there, it works in iterative (repetitive) steps to perform two tasks:\n",
        "\n",
        "1. Assign each data point to the closest corresponding centroid, using the any of the above distance measure.\n",
        "2. For each centroid, calculate the mean of the values of all the points belonging to it. The mean value becomes the new value of the centroid.\n",
        "\n",
        "Once step 2 is complete, all of the centroids have new values that correspond to the means of all of their corresponding points. These new points are put through steps *1* and *2* producing yet another set of centroid values. This process is repeated over and over until there is no change in the centroid values, meaning that they have been accurately grouped. Or, the process can be stopped when a previously determined maximum number of steps has been met.\n",
        "\n",
        "Now the problem is that, if any other random centroids were selected, the clusters could have been completely different. For this purpose, we pass an argument \"n_init\" to the algorithm so that it selects different combinations of random values as starting centroid points and gives us the best starting centroid combination depending on the **minimum value of inertia/within-cluster sum-of-squares**.\n",
        "\n",
        "***INERTIA :*** \n",
        "\n",
        "![alt text](https://i.ibb.co/kKctMzj/Capture1.png)\n",
        "\n",
        "Inertia is calculated by taking the sum of squares of distance of each data point from the centroid value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL69LpIygQyh",
        "colab_type": "text"
      },
      "source": [
        "***Please UpVote if you like the work!!!***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgBN5abCgUh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}